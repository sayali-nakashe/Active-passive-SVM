{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning and Passive Learning using Support Vector Machines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project , I have studied various methods for classification of data on the Bank Note Authentication Data Set. This is a binary classification problem. <br>\n",
    "\n",
    "The methods are listed below: <br>\n",
    "1) Active Learning using Support Vector Machines <br>\n",
    "--Used an L-1 penalized SVM for classification <br>\n",
    "2) Passive Learning using Support Vector Machines <br>\n",
    "--Used an L-1 penalized SVM for classification <br>\n",
    "\n",
    "<b>Banknote authentication Data Set </b> <br>\n",
    "Data were extracted from images that were taken for \n",
    "the evaluation of an authentication procedure for banknotes.\n",
    "\n",
    "The data is downloaded from : \n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "Attribute Information: <br>\n",
    "1. variance of Wavelet Transformed image (continuous)  <br>\n",
    "2. skewness of Wavelet Transformed image (continuous)  <br>\n",
    "3. curtosis of Wavelet Transformed image (continuous)  <br>\n",
    "4. entropy of image (continuous)  <br>\n",
    "5. class (integer)  <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_banknote_authentication.csv',names=['variance','skewness','kurtosis','entropy','class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  kurtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop('class',axis=1)\n",
    "Y=data.loc[:,'class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Monte-Carlo Simulation -\n",
    "#### Repeating each of the following two procedures 50 times. We will have 50 errors for 90 SVMs per each procedure.\n",
    "#### Training a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Selecting the penalty parameter using 10-fold cross validation.We repeat this process by adding 10 other randomly selected data points to the pool, until we use all the 900 points without replacing the samples back into the training set at each step. We will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. We have then implemented passive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C':[0.01,0.1,1,10,100]}\n",
    "final_result=pd.DataFrame()\n",
    "avg_error_passive=[]\n",
    "\n",
    "for p in range(0,50):\n",
    "    dataset= pd.read_csv('data_banknote_authentication.csv',names=['variance','skewness','kurtosis','entropy','class'])\n",
    "    full_test_data = dataset.sample(472,random_state=random.randint(0,100))\n",
    "    test_ind = full_test_data.index\n",
    "    full_test_data = full_test_data.reset_index(drop=True)\n",
    "    full_test_data_x = full_test_data.drop('class',axis=1)\n",
    "    full_test_data_y = full_test_data['class'] \n",
    "    train_data = dataset.drop(index=test_ind)\n",
    "\n",
    "    err=[]\n",
    "    new_train = pd.DataFrame()\n",
    "    \n",
    "    str_k = StratifiedKFold(n_splits=90)\n",
    "    X_train_data= train_data.drop(['class'],axis=1)\n",
    "    y_train_data = train_data['class']\n",
    "    y_train_data = y_train_data.reset_index(drop=True)\n",
    "    X_train_data = X_train_data.reset_index(drop=True)\n",
    "\n",
    "    for train_index,test_index in str_k.split(X_train_data,y_train_data):\n",
    "        sample_x,sample_y=X_train_data.iloc[test_index], y_train_data.iloc[test_index]\n",
    "        tr_sample_df= pd.concat([sample_x,sample_y],axis=1)\n",
    "        new_train = pd.concat([new_train,tr_sample_df],axis=0)\n",
    "        new_train_x = new_train.drop('class',axis=1)\n",
    "        new_train_y = new_train['class']\n",
    "        \n",
    "        # Training SVM with randomly selected Datapoints\n",
    "        l_svc=LinearSVC(penalty='l1',dual=False,max_iter=200000)\n",
    "        l_svc_passive = GridSearchCV(l_svc, params,cv=10,iid=1)\n",
    "        l_svc_passive.fit(new_train_x,new_train_y)\n",
    "        \n",
    "        pred_y_passive = l_svc_passive.predict(full_test_data_x)\n",
    "        err.append(1-accuracy_score(full_test_data_y,pred_y_passive))\n",
    "    \n",
    "    error_values = pd.Series(err)\n",
    "    final_result.insert(loc=p, column=p, value=error_values)\n",
    "    avg_error_passive=final_result.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM Number</th>\n",
       "      <th>Number of Samples</th>\n",
       "      <th>AVG Test Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.155720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.043432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.030720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0.020127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>0.038136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>90</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>120</td>\n",
       "      <td>0.020127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>130</td>\n",
       "      <td>0.013771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>140</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>0.013771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>160</td>\n",
       "      <td>0.015890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>170</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>180</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>190</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>210</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>220</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>230</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>240</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>250</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>260</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>270</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>280</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>290</td>\n",
       "      <td>0.013771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "      <td>0.013771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>610</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>620</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>630</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>640</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>650</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>660</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>670</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>680</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>690</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>700</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>710</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>720</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>730</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>740</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>750</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>760</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>770</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>780</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>790</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>800</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>810</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>820</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>830</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>840</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>850</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>860</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>870</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>880</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>890</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>900</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SVM Number  Number of Samples  AVG Test Error\n",
       "0            1                 10        0.155720\n",
       "1            2                 20        0.043432\n",
       "2            3                 30        0.033898\n",
       "3            4                 40        0.030720\n",
       "4            5                 50        0.020127\n",
       "5            6                 60        0.038136\n",
       "6            7                 70        0.016949\n",
       "7            8                 80        0.019068\n",
       "8            9                 90        0.014831\n",
       "9           10                100        0.012712\n",
       "10          11                110        0.012712\n",
       "11          12                120        0.020127\n",
       "12          13                130        0.013771\n",
       "13          14                140        0.011653\n",
       "14          15                150        0.013771\n",
       "15          16                160        0.015890\n",
       "16          17                170        0.012712\n",
       "17          18                180        0.012712\n",
       "18          19                190        0.011653\n",
       "19          20                200        0.010593\n",
       "20          21                210        0.009534\n",
       "21          22                220        0.010593\n",
       "22          23                230        0.012712\n",
       "23          24                240        0.014831\n",
       "24          25                250        0.014831\n",
       "25          26                260        0.014831\n",
       "26          27                270        0.014831\n",
       "27          28                280        0.014831\n",
       "28          29                290        0.013771\n",
       "29          30                300        0.013771\n",
       "..         ...                ...             ...\n",
       "60          61                610        0.009534\n",
       "61          62                620        0.009534\n",
       "62          63                630        0.009534\n",
       "63          64                640        0.009534\n",
       "64          65                650        0.009534\n",
       "65          66                660        0.009534\n",
       "66          67                670        0.009534\n",
       "67          68                680        0.009534\n",
       "68          69                690        0.009534\n",
       "69          70                700        0.009534\n",
       "70          71                710        0.009534\n",
       "71          72                720        0.009534\n",
       "72          73                730        0.009534\n",
       "73          74                740        0.009534\n",
       "74          75                750        0.009534\n",
       "75          76                760        0.009534\n",
       "76          77                770        0.009534\n",
       "77          78                780        0.009534\n",
       "78          79                790        0.009534\n",
       "79          80                800        0.009534\n",
       "80          81                810        0.009534\n",
       "81          82                820        0.009534\n",
       "82          83                830        0.009534\n",
       "83          84                840        0.009534\n",
       "84          85                850        0.009534\n",
       "85          86                860        0.009534\n",
       "86          87                870        0.009534\n",
       "87          88                880        0.009534\n",
       "88          89                890        0.009534\n",
       "89          90                900        0.009534\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_num=range(1,91)\n",
    "samples_num=list(range(10,910,10))\n",
    "result=pd.DataFrame({\"SVM Number\":svm_num,\"Number of Samples\":samples_num,\"AVG Test Error\":avg_error_passive})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Monte-Carlo Simulation -\n",
    "#### Repeating each of the following two procedures 50 times. We will have 50 errors for 90 SVMs per each procedure.\n",
    "#### Training a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Selecting the penalty parameter using 10-fold cross validation.We choose the 10 closest data points in the training set to the margin of the SVM and adding them to the pool without replacing the samples back into the training set at each step. Repeat this process until all training data is used.We will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. We have then implemented active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1):\n",
    "    data = pd.read_csv('data_banknote_authentication.csv',names=['variance','skewness','curtosis','entropy','class'])\n",
    "    X=data.drop('class',axis=1)\n",
    "    Y=data.loc[:,'class']\n",
    "    s_split=StratifiedShuffleSplit(n_splits=5, test_size=472,random_state=0)\n",
    "    for train_index, test_index in s_split.split(X, Y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = Y.loc[train_index], Y.loc[test_index]\n",
    "    \n",
    "    X_train=X_train.reset_index(drop=True)\n",
    "    X_test=X_test.reset_index(drop=True)\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_test=y_test.reset_index(drop=True)\n",
    "    \n",
    "    new_train = pd.DataFrame()\n",
    "    test_errs = []\n",
    "    full_train_data = pd.concat([X_train,y_train],axis=1)\n",
    "    \n",
    "    tr_sample = full_train_data.sample(n=10,replace=False)\n",
    "         \n",
    "    new_train = pd.concat([new_train,tr_sample], axis = 0)\n",
    "   \n",
    "    full_train_data.drop(index=tr_sample.index.tolist(),inplace=True)\n",
    "    \n",
    "    new_train_x= new_train.loc[:,new_train.columns[new_train.columns != 'class']]\n",
    "    new_train_y= new_train.loc[:,'class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C':[0.01,0.1,1,10,100,1000]}\n",
    "avg_test_error1 = [[0 for x in range(0,50)] for y in range(0,90)] \n",
    "best_c1=[[0 for x in range(0,50)] for y in range(0,90)] \n",
    "final = pd.DataFrame()\n",
    "\n",
    "for p in range(0,50):\n",
    "    dataset= pd.read_csv('data_banknote_authentication.csv',names=['variance','skewness','kurtosis','entropy','class'])\n",
    "    full_test_data = dataset.sample(472,random_state=random.randint(0,100))\n",
    "    test_ind = full_test_data.index\n",
    "    train_data = data.drop(index=test_ind)\n",
    "    full_test_data = full_test_data.reset_index(drop=True)\n",
    "    full_test_data_x = full_test_data.drop('class',axis=1)\n",
    "    full_test_data_y = full_test_data['class'] \n",
    "\n",
    "    str_k = StratifiedKFold(n_splits=90)\n",
    "    X_train_data= train_data.drop(['class'],axis=1)\n",
    "    train_data_y = train_data['class']\n",
    "    train_data_y = train_data_y.reset_index(drop=True)\n",
    "    X_train_data = X_train_data.reset_index(drop=True)\n",
    " \n",
    "    new_train=pd.DataFrame()\n",
    "      \n",
    "    for train_index,test_index in str_k.split(X_train_data,train_data_y):\n",
    "        sample_x,sample_y=X_train_data.iloc[test_index], train_data_y.iloc[test_index]\n",
    "        tr_sample_df= pd.concat([sample_x,sample_y],axis=1)\n",
    "        new_train = pd.concat([new_train,tr_sample_df],axis=0)\n",
    "        break\n",
    "        \n",
    "    for i in range(0,90):\n",
    "        X_train_data= X_train_data.reset_index(drop=True)\n",
    "        \n",
    "        l1_svc=LinearSVC(penalty='l1',dual=False,max_iter=200000)\n",
    "        l_svc_active = GridSearchCV(l1_svc, params,cv=10,iid=1)\n",
    "        l_svc_active.fit(new_train_x,new_train_y)\n",
    "        \n",
    "        closest_pts = (l_svc_active.decision_function(X_train_data))\n",
    "        closest_pts=np.abs(closest_pts)\n",
    "        distances =  pd.DataFrame(closest_pts)\n",
    "        distances = distances.sort_values(0)\n",
    "        distances = distances.iloc[0:10,]\n",
    "        distances_index = distances.index\n",
    "        \n",
    "        tr_sample  =  train_data.iloc[distances_index,:]\n",
    "        X_train_data= X_train_data.drop(index = distances_index)\n",
    "\n",
    "        pred_y = l_svc_active.predict(full_test_data_x)\n",
    "        \n",
    "        new_train = pd.concat([new_train,tr_sample],axis=0)\n",
    "        new_train.reset_index(drop=True,inplace=True)\n",
    "        new_train_x = new_train.drop('class',axis=1)\n",
    "        new_train_y = new_train['class']\n",
    "  \n",
    "        avg_test_error1[i][p]=1-accuracy_score(full_test_data_y,pred_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from collections import Counter\n",
    "\n",
    "svm_test_errors1=[]\n",
    "final_best_c1=[]\n",
    "\n",
    "for j in range(0,90):\n",
    "    svm_test_errors1.append(np.round(mean(avg_test_error1[j]),3))\n",
    "    cntr = Counter(best_c1[j])\n",
    "    value, count = cntr.most_common()[0]\n",
    "    final_best_c1.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_num=range(1,91)\n",
    "samples_num=list(range(10,910,10))\n",
    "result1=pd.DataFrame({\"SVM Number\":svm_num,\"Number of Samples\":samples_num,\"AVG Test Error\":svm_test_errors1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM Number</th>\n",
       "      <th>Number of Samples</th>\n",
       "      <th>AVG Test Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>0.0137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>90</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>0.0107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>130</td>\n",
       "      <td>0.0137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>180</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>190</td>\n",
       "      <td>0.0107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>210</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>220</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>230</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>240</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>250</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>260</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>270</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>280</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>290</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>610</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>620</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>630</td>\n",
       "      <td>0.0097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>640</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>650</td>\n",
       "      <td>0.0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>660</td>\n",
       "      <td>0.0097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>670</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>680</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>700</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>710</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>720</td>\n",
       "      <td>0.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>730</td>\n",
       "      <td>0.0049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>740</td>\n",
       "      <td>0.0049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>750</td>\n",
       "      <td>0.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>760</td>\n",
       "      <td>0.0049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>770</td>\n",
       "      <td>0.0049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>780</td>\n",
       "      <td>0.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>790</td>\n",
       "      <td>0.0049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>800</td>\n",
       "      <td>0.0049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>810</td>\n",
       "      <td>0.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>820</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>830</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>840</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>850</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>860</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>870</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>880</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>890</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>900</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SVM Number  Number of Samples  AVG Test Error\n",
       "0            1                 10          0.1340\n",
       "1            2                 20          0.0520\n",
       "2            3                 30          0.0180\n",
       "3            4                 40          0.0187\n",
       "4            5                 50          0.0177\n",
       "5            6                 60          0.0207\n",
       "6            7                 70          0.0137\n",
       "7            8                 80          0.0107\n",
       "8            9                 90          0.0117\n",
       "9           10                100          0.0117\n",
       "10          11                110          0.0107\n",
       "11          12                120          0.0107\n",
       "12          13                130          0.0137\n",
       "13          14                140          0.0117\n",
       "14          15                150          0.0117\n",
       "15          16                160          0.0157\n",
       "16          17                170          0.0117\n",
       "17          18                180          0.0117\n",
       "18          19                190          0.0107\n",
       "19          20                200          0.0107\n",
       "20          21                210          0.0117\n",
       "21          22                220          0.0117\n",
       "22          23                230          0.0117\n",
       "23          24                240          0.0117\n",
       "24          25                250          0.0117\n",
       "25          26                260          0.0117\n",
       "26          27                270          0.0157\n",
       "27          28                280          0.0157\n",
       "28          29                290          0.0117\n",
       "29          30                300          0.0117\n",
       "..         ...                ...             ...\n",
       "60          61                610          0.0089\n",
       "61          62                620          0.0089\n",
       "62          63                630          0.0097\n",
       "63          64                640          0.0089\n",
       "64          65                650          0.0103\n",
       "65          66                660          0.0097\n",
       "66          67                670          0.0089\n",
       "67          68                680          0.0089\n",
       "68          69                690          0.0097\n",
       "69          70                700          0.0089\n",
       "70          71                710          0.0069\n",
       "71          72                720          0.0057\n",
       "72          73                730          0.0049\n",
       "73          74                740          0.0049\n",
       "74          75                750          0.0057\n",
       "75          76                760          0.0049\n",
       "76          77                770          0.0049\n",
       "77          78                780          0.0057\n",
       "78          79                790          0.0049\n",
       "79          80                800          0.0049\n",
       "80          81                810          0.0095\n",
       "81          82                820          0.0067\n",
       "82          83                830          0.0067\n",
       "83          84                840          0.0067\n",
       "84          85                850          0.0067\n",
       "85          86                860          0.0067\n",
       "86          87                870          0.0067\n",
       "87          88                880          0.0067\n",
       "88          89                890          0.0067\n",
       "89          90                900          0.0067\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve by Monte-Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "passive,=plt.plot(samples_num, avg_error_passive,label='Passive',linewidth=3)\n",
    "active,=plt.plot(samples_num, svm_test_errors1,label='Active',linewidth=3)\n",
    "plt.legend(handles=[active,passive])\n",
    "plt.ylabel('Average Test error')\n",
    "plt.xlabel('Number of Training Instances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, we can say that Active learning reaches minimum error faster then Passive Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
